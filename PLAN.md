Rust vs. Python ML Benchmarking: Project PlanThis document provides a complete plan for executing the six-phase benchmarking methodology comparing Rust and Python machine learning ecosystems. It outlines the project's directory structure, the datasets to be used, and a detailed implementation guide for each benchmark task.1. Project Directory StructureA well-organized monorepo is essential for reproducibility. The following structure separates code, data, configuration, and results, making the project easy to navigate and automate.rust-ml-benchmark/
├── .github/              # CI/CD workflows (e.g., GitHub Actions) for automated testing.
├── data/                 # Raw and processed datasets.
│   ├── raw/              # Original downloaded data files (e.g., .csv, .tar.gz).
│   └── processed/        # Cleaned, split, and standardized data ready for use.
├── docker/               # Container definitions for consistent environments.
│   ├── python.Dockerfile # Sets up the Python environment with all dependencies.
│   └── rust.Dockerfile   # Sets up the Rust environment with all dependencies.
├── experiments/          # All benchmark source code.
│   ├── python/           # Python implementations for each task.
│   │   ├── classical_ml.py
│   │   ├── deep_learning.py
│   │   └── llm_inference.py
│   └── rust/             # Rust implementations, organized as Cargo projects.
│       ├── classical_ml/
│       ├── deep_learning/
│       └── llm_inference/
├── analysis/             # Scripts for statistical analysis and visualization.
│   └── analyze_results.py
├── results/              # Raw, timestamped output from benchmark runs (JSON format).
├── scripts/              # Helper scripts for automation and orchestration.
│   ├── run_benchmark.sh  # Main script to execute a single benchmark run.
│   └── monitor_resources.sh # Background script to log resource usage.
└── nextflow.config       # Nextflow pipeline configuration for orchestrating all runs.

2. Dataset Selection and PreparationThe following datasets have been selected for their widespread use, clear licensing, and suitability for the specified tasks.Task CategoryDatasetSource / LoaderPreparation & Storage (data/ directory)Classical MLCalifornia Housingsklearn.datasets.fetch_california_housingFetch using the scikit-learn loader, then save the features and target as california_housing.csv in data/processed/.Classical MLDigitssklearn.datasets.load_digitsLoad the dataset, then save the features and target as digits.csv in data/processed/.Classical MLGenerated Blobssklearn.datasets.make_blobsNo storage needed. Generate this data on-the-fly within the benchmark scripts to ensure identical distributions.Deep LearningCIFAR-10torchvision.datasets.CIFAR10Download using torchvision to data/raw/. The data loaders in both Python and Rust will read directly from this location.LLM InferenceSQuAD 2.0 (dev set)Hugging Face Datasets HubDownload the validation set using the datasets library. Save it as squad_dev.json in data/processed/.LLM InferenceCustom PromptsN/ACreate a simple text file text_generation_prompts.txt in data/processed/ with 15-20 diverse prompts for text generation tasks.3. Implementation Plan by TaskThis section details the specific algorithms, models, and frameworks to be implemented for each benchmark.3.1. Classical Machine Learning (classical_ml)Objective: Compare CPU-bound performance on traditional ML algorithms.TaskAlgorithmPython (scikit-learn)Rust (linfa / smartcore)DatasetKey ParametersRegressionLinear RegressionLinearRegressionlinfa-linear::LinearRegressionCalifornia HousingNone (Defaults)ClassificationSupport Vector MachineSVC(kernel='linear')linfa-svm::SvmDigitskernel='linear'ClusteringK-MeansKMeans(n_clusters=8)linfa-clustering::KMeansGenerated Blobsn_clusters=8, random_state=42Note: The linfa ecosystem is modular. Implementations will require adding specific crates like linfa-linear, linfa-svm, and linfa-clustering to the Cargo.toml file.3.2. Deep Learning (deep_learning)Objective: Compare GPU-bound training performance for a standard computer vision task.ComponentSpecificationPython (PyTorch)Rust (tch-rs, burn, candle)TaskImage Classification TrainingTrain a CNN from scratch.Implement the training loop in each framework.DatasetCIFAR-10torchvision.datasets.CIFAR10Use built-in or custom dataset loaders.ArchitectureStandard CNNnn.Sequential with:<br/>- Conv2d(3, 32)<br/>- ReLU<br/>- MaxPool2d<br/>- Conv2d(32, 64)<br/>- ReLU<br/>- MaxPool2d<br/>- Flatten<br/>- Linear(..., 512)<br/>- ReLU<br/>- Linear(512, 10)Replicate this exact architecture in each Rust framework.OptimizerAdamtorch.optim.AdamUse the Adam optimizer in each framework.Hyperparameterslearning_rate=0.001, batch_size=64, epochs=10Set these values identically across all implementations.3.3. LLM Inference (llm_inference)Objective: Compare latency and throughput for pre-trained language model inference.ModelTaskPython (transformers)Rust (tch-rs, candle, ort)Scenario / DatasetBERTQuestion AnsweringBertForQuestionAnsweringLoad the same pre-trained model checkpoint in each framework.Measure single-request latency on questions from the SQuAD 2.0 dev set.GPT-2Text GenerationGPT2LMHeadModelLoad the same pre-trained model checkpoint.Measure tokens/second generating 150 tokens from the Custom Prompts file.ONNXAll TasksN/Aort crateConvert both BERT and GPT-2 models to ONNX format and benchmark inference using the ort crate for a deployment-focused comparison.